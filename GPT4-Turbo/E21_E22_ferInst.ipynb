{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89da5844",
   "metadata": {},
   "source": [
    "# Experiment E21 E22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68886320",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d66c8a-7ae0-4f34-afba-17d46fff9988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from tenacity import (retry, stop_after_attempt, wait_random_exponential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34ea20",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b7c74f-7d99-488d-8f7d-57830de55939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List that hold the records\n",
    "data = []\n",
    "\n",
    "# Opening the CSV file\n",
    "with open('Dataset/ferretti_instrument.csv', mode ='r')as file:\n",
    "   \n",
    "  # reading the CSV file\n",
    "  csvFile = csv.reader(file)\n",
    "  \n",
    "  # displaying the contents of the CSV file\n",
    "  for lines in csvFile:\n",
    "      data.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ea381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving the results \n",
    "fields_E21 = ['predicate','role','sentence_1','fit_score_1','sentence_2','fit_score_2','sentence_3','fit_score_3','sentence_4','fit_score_4','sentence_5','fit_score_5','actual_fit','exp_fit']\n",
    "filename_E21 = 'Result_final/E21_ferInst_results.csv'\n",
    "    \n",
    "fields_E22 = ['predicate','role','sentence_1','fit_score_1','sentence_2','fit_score_2','sentence_3','fit_score_3','sentence_4','fit_score_4','sentence_5','fit_score_5','actual_fit','exp_fit']\n",
    "filename_E22 = 'Result_final/E22_ferInst_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355332e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filename_E21):\n",
    "    with open(filename_E21, 'a') as csvfile: \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields_E21)\n",
    "\n",
    "if not os.path.exists(filename_E22):\n",
    "    with open(filename_E22, 'a') as csvfile: \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields_E22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994c6e4",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cc367a-93c4-4c84-a888-57e326108a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ece7cf8-4883-4147-8cf8-caec2f6d2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_engine= \"gpt-4-1106-preview\"\n",
    "temp = 0.0\n",
    "top_p = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79987da5-881e-4d78-a3e7-cc54dd464533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_with_functions(conversation: [{}]):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=model_engine,\n",
    "      messages=conversation,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      temperature= temp,\n",
    "      top_p = top_p,\n",
    "    )\n",
    " \n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884730a",
   "metadata": {},
   "source": [
    "### Help Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdbe591b-9146-47a9-a8c9-c579280141a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_generate_sentence = f\"Generate five sentences that semantically fit the given predicate '{predicate}', argument '{argument}', and the role of {roleType}. Reply only with a JSON list containing the key 'sentences' with the five sentences each is semantically coherent with the given predicate, argument, and role. \"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_generate_sentence})\n",
    "    response_generate_sentence = chat_with_functions(conversation = conversation)\n",
    "\n",
    "    '''TRACING'''\n",
    "    print('response_generate_sentence', response_generate_sentence)\n",
    "    \n",
    "    sentences = json.loads(response_generate_sentence)[\"sentences\"]\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": \",\".join(sentences)})\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39849e48-7444-4e2e-9356-d408b294912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_coherent (conversation: [{}], sentence: str):\n",
    "    prompt_check_semantic = f\"Is the given sentence '{sentence} semantically coherent? Reply only with JSON list has the key 'Is Fit' containing 'Yes' or 'No'.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_check_semantic})\n",
    "    response_check_semantic = chat_with_functions(conversation = conversation) \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_check_semantic', response_check_semantic)\n",
    "    \n",
    "    check_semantic = json.loads(response_check_semantic)[\"Is Fit\"]  \n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{check_semantic}\"})\n",
    "\n",
    "    if 'No' in check_semantic:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d54f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_fit_scoring_with_senetence(conversation: [{}], sentence: str, predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score_textual= f\"Given the following sentence '{sentence}', for the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}?  Reply only with JSON list has the key 'Fit score' containing a value that is  is one of 'Near-Perfect', 'High', 'Medium', 'Low' or 'Near-Impossible'.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score_textual})\n",
    "    response_fit_score_textual =  chat_with_functions(conversation = conversation)                \n",
    "\n",
    "    '''TRACING'''\n",
    "    print('response_fit_score_textual',response_fit_score_textual)\n",
    "    \n",
    "    fit_score_textual = json.loads(response_fit_score_textual)['Fit score']\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score_textual}\"})\n",
    "    fit_score = textual_to_numerical_scale(fit_score_textual)\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c4f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_fit_scoring_with_senetence(conversation: [{}], sentence: str, predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score= f\"Given the following sentence '{sentence}', for the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}?  Reply only with JSON list has the key 'Fit score' containing a value that is a float number from 0 to 1.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score})\n",
    "    response_fit_score =  chat_with_functions(conversation = conversation)                \n",
    "\n",
    "    '''TRACING'''\n",
    "    print('response_fit_score', response_fit_score)\n",
    "    \n",
    "    fit_score = float(json.loads(response_fit_score)['Fit score'])\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score}\"})\n",
    "\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8210ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_fit_scoring_with_backoff(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score_textual= f\"Given the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}?  Reply only with JSON list has the key 'Fit score' containing a value that is  is one of 'Near-Perfect', 'High', 'Medium', 'Low' or 'Near-Impossible'.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score_textual})\n",
    "    response_fit_score_textual =  chat_with_functions(conversation = conversation)                \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_fit_score_textual', response_fit_score_textual)\n",
    "    \n",
    "    fit_score_textual = json.loads(response_fit_score_textual)['Fit score']\n",
    "    ## CHECK\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score_textual}\"})\n",
    "    fit_score = textual_to_numerical_scale(fit_score_textual)\n",
    "    fit_score /= 2\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90be40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_fit_scoring_with_backoff(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score= f\"Given the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}? Reply only with JSON list has the key 'Fit score' containing a value that is a float number from 0 to 1. \"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score})\n",
    "    response_fit_score = chat_with_functions(conversation = conversation)                 \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_fit_score', response_fit_score)\n",
    "    \n",
    "    fit_score = float(json.loads(response_fit_score)['Fit score'])\n",
    "    ## CHECK\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score}\"})\n",
    "    fit_score /= 2\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a84af91-3441-452f-93c5-2bcbf047f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowExist(result_file_name, predicate, argument):\n",
    "    with open(result_file_name, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        for line in csvFile:\n",
    "            if predicate in line and argument in line:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae3875cd-bd26-4192-87b7-3f63fd8e3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_to_numerical_scale(level):\n",
    "    if \"Near-Impossible\" in level:\n",
    "        return 0.0\n",
    "    elif \"Low\" in level:\n",
    "        return 0.25\n",
    "    elif \"Medium\" in level:\n",
    "        return 0.5\n",
    "    elif \"High\" in level:\n",
    "        return 0.75\n",
    "    elif \"Near-Perfect\" in level:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c2cf",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e11f45-92e3-41d4-bd99-80d36dc6ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Record\n",
      "--------------------------------------\n",
      "serve plate\n",
      "response_generate_sentence {\n",
      "  \"sentences\": [\n",
      "    \"The waiter used a shiny silver plate to serve the hors d'oeuvres.\",\n",
      "    \"During the banquet, a beautifully decorated plate was utilized to serve the guests the main course.\",\n",
      "    \"She skillfully served the appetizers on a rotating glass plate to all the attendees.\",\n",
      "    \"For the dessert presentation, the chef chose a chilled marble plate to serve the delicate pastries.\",\n",
      "    \"At the family dinner, a large ceramic plate was passed around to serve slices of homemade pizza.\"\n",
      "  ]\n",
      "}\n",
      "[\"The waiter used a shiny silver plate to serve the hors d'oeuvres.\", 'During the banquet, a beautifully decorated plate was utilized to serve the guests the main course.', 'She skillfully served the appetizers on a rotating glass plate to all the attendees.', 'For the dessert presentation, the chef chose a chilled marble plate to serve the delicate pastries.', 'At the family dinner, a large ceramic plate was passed around to serve slices of homemade pizza.']\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\"Fit score\": \"Near-Perfect\"}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "serve plate Instrument fit_score_final 1.0 1.0\n",
      "New Record\n",
      "--------------------------------------\n",
      "serve tray\n",
      "response_generate_sentence {\n",
      "  \"sentences\": [\n",
      "    \"The waiter used a tray to serve the hors d'oeuvres at the gala.\",\n",
      "    \"During the flight, the attendant will serve drinks on a tray to the passengers.\",\n",
      "    \"At the high tea event, the hostess served scones and clotted cream on a silver tray.\",\n",
      "    \"The caterer served the appetizers on a tray to keep them organized and presentable.\",\n",
      "    \"In the dining car of the train, the server came by to serve breakfast on a neatly arranged tray.\"\n",
      "  ]\n",
      "}\n",
      "[\"The waiter used a tray to serve the hors d'oeuvres at the gala.\", 'During the flight, the attendant will serve drinks on a tray to the passengers.', 'At the high tea event, the hostess served scones and clotted cream on a silver tray.', 'The caterer served the appetizers on a tray to keep them organized and presentable.', 'In the dining car of the train, the server came by to serve breakfast on a neatly arranged tray.']\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "serve tray Instrument fit_score_final 1.0 1.0\n",
      "New Record\n",
      "--------------------------------------\n",
      "serve glass\n",
      "response_generate_sentence {\n",
      "  \"sentences\": [\n",
      "    \"The waiter served the wine using a crystal glass.\",\n",
      "    \"She served the guests water in a chilled glass.\",\n",
      "    \"During the banquet, they served the punch with an ornate glass ladle.\",\n",
      "    \"He served the homemade lemonade in a glass pitcher.\",\n",
      "    \"The bartender served the cocktail through a glass mixing rod.\"\n",
      "  ]\n",
      "}\n",
      "['The waiter served the wine using a crystal glass.', 'She served the guests water in a chilled glass.', 'During the banquet, they served the punch with an ornate glass ladle.', 'He served the homemade lemonade in a glass pitcher.', 'The bartender served the cocktail through a glass mixing rod.']\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"High\"\n",
      "}\n",
      "0.75\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.9\n",
      "}\n",
      "0.9\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Low\"\n",
      "}\n",
      "0.25\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.2\n",
      "}\n",
      "0.2\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"High\"\n",
      "}\n",
      "0.75\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.8\n",
      "}\n",
      "0.8\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"No\"\n",
      "}\n",
      "He served the homemade lemonade in a glass pitcher. \t Not Sematically Fit Sentence\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Low\"\n",
      "}\n",
      "0.125\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.2\n",
      "}\n",
      "0.1\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"High\"\n",
      "}\n",
      "0.75\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.8\n",
      "}\n",
      "0.8\n",
      "serve glass Instrument fit_score_final 0.56 0.53\n",
      "New Record\n",
      "--------------------------------------\n",
      "serve platter\n",
      "response_generate_sentence {\n",
      "  \"sentences\": [\n",
      "    \"The waiter served the hors d'oeuvres on a shiny silver platter.\",\n",
      "    \"During the banquet, the staff served the roasted turkey on a large decorative platter.\",\n",
      "    \"At the garden party, sandwiches were served on a ceramic platter adorned with floral patterns.\",\n",
      "    \"The caterer served the assortment of cheeses and fruits on a wooden platter to the guests.\",\n",
      "    \"For the holiday feast, the family served the main course on an heirloom platter that was passed down through generations.\"\n",
      "  ]\n",
      "}\n",
      "[\"The waiter served the hors d'oeuvres on a shiny silver platter.\", 'During the banquet, the staff served the roasted turkey on a large decorative platter.', 'At the garden party, sandwiches were served on a ceramic platter adorned with floral patterns.', 'The caterer served the assortment of cheeses and fruits on a wooden platter to the guests.', 'For the holiday feast, the family served the main course on an heirloom platter that was passed down through generations.']\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\"Fit score\": \"Near-Perfect\"}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Near-Perfect\"\n",
      "}\n",
      "1.0\n",
      "response_fit_score {\n",
      "  \"Fit score\": 1.0\n",
      "}\n",
      "1.0\n",
      "serve platter Instrument fit_score_final 1.0 1.0\n",
      "New Record\n",
      "--------------------------------------\n",
      "serve bucket\n",
      "response_generate_sentence {\n",
      "  \"sentences\": [\n",
      "    \"The waiter used a bucket to serve the bottle of champagne.\",\n",
      "    \"During the beach party, they served drinks from a bucket filled with ice.\",\n",
      "    \"In the rustic restaurant, soup was served in a bucket for a unique dining experience.\",\n",
      "    \"The bartender served the cocktail using a bucket as a novelty mixing vessel.\",\n",
      "    \"At the medieval-themed event, the stew was served out of a large iron bucket.\"\n",
      "  ]\n",
      "}\n",
      "['The waiter used a bucket to serve the bottle of champagne.', 'During the beach party, they served drinks from a bucket filled with ice.', 'In the rustic restaurant, soup was served in a bucket for a unique dining experience.', 'The bartender served the cocktail using a bucket as a novelty mixing vessel.', 'At the medieval-themed event, the stew was served out of a large iron bucket.']\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"No\"\n",
      "}\n",
      "The waiter used a bucket to serve the bottle of champagne. \t Not Sematically Fit Sentence\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Low\"\n",
      "}\n",
      "0.125\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.2\n",
      "}\n",
      "0.1\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"High\"\n",
      "}\n",
      "0.75\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.8\n",
      "}\n",
      "0.8\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"Medium\"\n",
      "}\n",
      "0.5\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.6\n",
      "}\n",
      "0.6\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"High\"\n",
      "}\n",
      "0.75\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.7\n",
      "}\n",
      "0.7\n",
      "response_check_semantic {\n",
      "  \"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual {\n",
      "  \"Fit score\": \"High\"\n",
      "}\n",
      "0.75\n",
      "response_fit_score {\n",
      "  \"Fit score\": 0.8\n",
      "}\n",
      "0.8\n",
      "serve bucket Instrument fit_score_final 0.6 0.57\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):  # This must be for all datat 'len(data)' but for now just 5 records \n",
    "    \n",
    "    '''Data Extraction'''\n",
    "    predicate = data[i][0]\n",
    "    argument = data[i][1]\n",
    "    roleType = 'Instrument'\n",
    "    actual_fit = data[i][2]\n",
    "\n",
    "    '''Check if this record is exists already'''\n",
    "    IsExist_E21 = False\n",
    "    IsExist_E22 = False \n",
    "    \n",
    "    if os.path.exists(filename_E21) and  os.path.exists(filename_E22):\n",
    "        IsExist_E21 = rowExist(filename_E21, predicate,argument)\n",
    "        IsExist_E22 = rowExist(filename_E22, predicate,argument)\n",
    "\n",
    "        if IsExist_E21 and IsExist_E22:\n",
    "            continue\n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('New Record')\n",
    "    print('--------------------------------------')\n",
    "    print(predicate,argument)\n",
    "    \n",
    "    '''Result Preparation'''    \n",
    "    if not IsExist_E21:\n",
    "        result_E21 = [predicate,argument]\n",
    "    if not IsExist_E22:\n",
    "        result_E22 = [predicate,argument] \n",
    "\n",
    "    '''Chat Completion Needed Parameter'''\n",
    "    system_prompt = \"You are a linguist who can understand the semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument and the semantic role.\"\n",
    "    conversation = [{\"role\": \"system\", \"content\": system_prompt}] \n",
    "\n",
    "    '''Sentence Generation Part'''\n",
    "    sentence_no = 5\n",
    "    sentences = generate_sentence(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "    '''TRACING'''\n",
    "    print(sentences)\n",
    "    \n",
    "    '''Prompting'''\n",
    "    avg_E21 = 0\n",
    "    avg_E22 = 0\n",
    "\n",
    "    for j in sentences:\n",
    "\n",
    "        '''Checking Semantic Coherent'''\n",
    "        is_semantic = semantic_coherent(conversation= conversation, sentence= j)      \n",
    "            \n",
    "        '''Not Sematically Fit Sentence'''\n",
    "        if not is_semantic:\n",
    "\n",
    "            '''Back Off'''\n",
    "            \n",
    "            '''TRACING'''\n",
    "            print(j, '\\t Not Sematically Fit Sentence')\n",
    "            \n",
    "            # as the sentnece is not semantically fit, do not consider and add it as an empty in the result\n",
    "            if not IsExist_E21:\n",
    "                result_E21.append('')\n",
    "            if not IsExist_E22:\n",
    "                result_E22.append('')\n",
    "                    \n",
    "            if not IsExist_E22:\n",
    "                fit_score = textual_fit_scoring_with_backoff(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "                '''TRACING'''\n",
    "                print(fit_score)\n",
    "                \n",
    "                result_E22.append(fit_score)\n",
    "                avg_E22 += fit_score\n",
    "\n",
    "            if not IsExist_E21:\n",
    "                fit_score = numerical_fit_scoring_with_backoff(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "                '''TRACING'''\n",
    "                print(fit_score)\n",
    "                \n",
    "                result_E21.append(fit_score)\n",
    "                avg_E21 += fit_score\n",
    "\n",
    "        '''Sematically Fit Sentence'''\n",
    "        if is_semantic:    \n",
    "\n",
    "            \n",
    "            # consider the sentence\n",
    "            if not IsExist_E21:\n",
    "                result_E21.append(j)\n",
    "            if not IsExist_E22:\n",
    "                result_E22.append(j)\n",
    "\n",
    "            if not IsExist_E22:\n",
    "                fit_score = textual_fit_scoring_with_senetence(conversation= conversation, sentence= j, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "                '''TRACING'''\n",
    "                print(fit_score)\n",
    "                \n",
    "                result_E22.append(fit_score)\n",
    "                avg_E22 += fit_score\n",
    "\n",
    "            if not IsExist_E21:\n",
    "                fit_score = numerical_fit_scoring_with_senetence(conversation= conversation, sentence= j, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "                '''TRACING'''\n",
    "                print(fit_score)\n",
    "                \n",
    "                result_E21.append(fit_score)\n",
    "                avg_E21 += fit_score\n",
    "\n",
    "    '''Add the results'''\n",
    "    result_E21.append(actual_fit)\n",
    "    result_E21.append(round(avg_E21/sentence_no,2))\n",
    "\n",
    "    result_E22.append(actual_fit)\n",
    "    result_E22.append(round(avg_E22/sentence_no,2))\n",
    "\n",
    "    '''TRACING'''\n",
    "    print(predicate, argument, roleType, 'fit_score_final', round(avg_E21/sentence_no,2) ,  round(avg_E22/sentence_no,2))\n",
    "\n",
    "    '''ADD IT TO THE RESULT DATASET'''\n",
    "    if not IsExist_E21:\n",
    "        with open(filename_E21, 'a') as csvfile: \n",
    "            # creating a csv writer object  \n",
    "            csvwriter = csv.writer(csvfile) \n",
    "\n",
    "            # writing the data rows  \n",
    "            csvwriter.writerow(result_E21) \n",
    "        \n",
    "    if not IsExist_E22:\n",
    "        with open(filename_E22, 'a') as csvfile: \n",
    "            # creating a csv writer object  \n",
    "            csvwriter = csv.writer(csvfile) \n",
    "\n",
    "            # writing the data rows  \n",
    "            csvwriter.writerow(result_E22) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b9995-66aa-4461-ad2e-45f1bb0a9c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
