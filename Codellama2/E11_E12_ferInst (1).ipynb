{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89da5844",
   "metadata": {},
   "source": [
    "# Experiment E11 E12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68886320",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d66c8a-7ae0-4f34-afba-17d46fff9988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from tenacity import (retry, stop_after_attempt, wait_random_exponential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34ea20",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b7c74f-7d99-488d-8f7d-57830de55939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List that hold the records\n",
    "data = []\n",
    "\n",
    "# Opening the CSV file\n",
    "with open('Dataset/ferretti_instrument.csv', mode ='r')as file:\n",
    "   \n",
    "  # reading the CSV file\n",
    "  csvFile = csv.reader(file)\n",
    "  \n",
    "  # displaying the contents of the CSV file\n",
    "  for lines in csvFile:\n",
    "      data.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ea381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving the results \n",
    "fields_E21 = ['predicate','argument','actual_fit','exp_fit']\n",
    "filename_E21 = 'Result_final/E11_ferInst_llama13b_results.csv'\n",
    "    \n",
    "fields_E22 = ['predicate','argument','actual_fit','exp_fit']\n",
    "filename_E22 = 'Result_final/E12_ferInst_llama13b_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "355332e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filename_E21):\n",
    "    with open(filename_E21, 'a') as csvfile: \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields_E21)\n",
    "\n",
    "if not os.path.exists(filename_E22):\n",
    "    with open(filename_E22, 'a') as csvfile: \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields_E22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994c6e4",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cc367a-93c4-4c84-a888-57e326108a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://0.0.0.0:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ece7cf8-4883-4147-8cf8-caec2f6d2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_engine= \"codellama/CodeLlama-13b-Instruct-hf\"\n",
    "temp = 0.0\n",
    "top_p = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79987da5-881e-4d78-a3e7-cc54dd464533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_with_functions(conversation: [{}]):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=model_engine,\n",
    "      messages=conversation,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      temperature= temp,\n",
    "      top_p = top_p,\n",
    "    )\n",
    " \n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884730a",
   "metadata": {},
   "source": [
    "### Help Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8210ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_fit_scoring(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score_textual= f\"Given the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}?  Reply only with JSON list has the key 'Fit score' containing a value that is one of 'Near-Perfect', 'High', 'Medium', 'Low' or 'Near-Impossible'. Do not reply anything else rather than the JSON list.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score_textual})\n",
    "    response_fit_score_textual =  chat_with_functions(conversation = conversation)                \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_fit_score_textual', response_fit_score_textual)\n",
    "    \n",
    "    fit_score_textual = json.loads(response_fit_score_textual)['Fit score']\n",
    "    ## CHECK\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score_textual}\"})\n",
    "    fit_score = textual_to_numerical_scale(fit_score_textual)\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90be40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_fit_scoring(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score= f\"Given the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}? Reply only with JSON list has the key 'Fit score' containing a value that is a float number from 0 to 1. Do not reply anything else rather than the JSON list. \"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score})\n",
    "    response_fit_score = chat_with_functions(conversation = conversation)                 \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_fit_score', response_fit_score)\n",
    "    \n",
    "    fit_score = float(json.loads(response_fit_score)['Fit score'])\n",
    "    ## CHECK\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score}\"})\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a84af91-3441-452f-93c5-2bcbf047f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowExist(result_file_name, predicate, argument):\n",
    "    with open(result_file_name, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        for line in csvFile:\n",
    "            if predicate in line and argument in line:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3875cd-bd26-4192-87b7-3f63fd8e3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_to_numerical_scale(level):\n",
    "    if \"Near-Impossible\" in level:\n",
    "        return 0.0\n",
    "    elif \"Low\" in level:\n",
    "        return 0.25\n",
    "    elif \"Medium\" in level:\n",
    "        return 0.5\n",
    "    elif \"High\" in level:\n",
    "        return 0.75\n",
    "    elif \"Near-Perfect\" in level:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c2cf",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03e11f45-92e3-41d4-bd99-80d36dc6ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Record\n",
      "--------------------------------------\n",
      "serve platter\n",
      "response_fit_score_textual   {\n",
      "\"Fit score\": \"High\"\n",
      "}\n",
      "0.75\n",
      "response_fit_score   {\n",
      "\"Fit score\": 0.75\n",
      "}\n",
      "0.75\n",
      "serve platter Instrument fit_score_final 0.75 0.75\n",
      "New Record\n",
      "--------------------------------------\n",
      "serve bucket\n",
      "response_fit_score_textual   {\n",
      "\"Fit score\": \"High\"\n",
      "}\n",
      "0.75\n",
      "response_fit_score   {\n",
      "\"Fit score\": 0.75\n",
      "}\n",
      "0.75\n",
      "serve bucket Instrument fit_score_final 0.75 0.75\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):  # This must be for all datat 'len(data)' but for now just 5 records \n",
    "    \n",
    "    '''Data Extraction'''\n",
    "    predicate = data[i][0]\n",
    "    argument = data[i][1]\n",
    "    roleType = 'Instrument'\n",
    "    actual_fit = data[i][2]\n",
    "\n",
    "    '''Check if this record is exists already'''\n",
    "    IsExist_E21 = False\n",
    "    IsExist_E22 = False \n",
    "    \n",
    "    if os.path.exists(filename_E21) and  os.path.exists(filename_E22):\n",
    "        IsExist_E21 = rowExist(filename_E21, predicate,argument)\n",
    "        IsExist_E22 = rowExist(filename_E22, predicate,argument)\n",
    "\n",
    "        if IsExist_E21 and IsExist_E22:\n",
    "            continue\n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('New Record')\n",
    "    print('--------------------------------------')\n",
    "    print(predicate,argument)\n",
    "    \n",
    "    '''Result Preparation'''  \n",
    "    result_E21 = []\n",
    "    result_E22 = []\n",
    "    \n",
    "    if not IsExist_E21:\n",
    "        result_E21 = [predicate,argument]\n",
    "    if not IsExist_E22:\n",
    "        result_E22 = [predicate,argument] \n",
    "\n",
    "    '''Chat Completion Needed Parameter'''\n",
    "    system_prompt = \"You are a linguist who can understand the semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument and the semantic role.\"\n",
    "    conversation = [{\"role\": \"system\", \"content\": system_prompt}] \n",
    "\n",
    "    '''Prompting Part'''      \n",
    "                    \n",
    "    if not IsExist_E22:\n",
    "        '''FITING SCORING PART'''\n",
    "        fit_score_textual= textual_fit_scoring(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "        '''TRACING'''\n",
    "        print(fit_score_textual)\n",
    "                \n",
    "        result_E22.append(actual_fit)\n",
    "        result_E22.append(round(fit_score_textual,2))\n",
    "\n",
    "    if not IsExist_E21:\n",
    "        '''FITING SCORING PART'''\n",
    "        fit_score_numerical = numerical_fit_scoring(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "        '''TRACING'''\n",
    "        print(fit_score_numerical)\n",
    "\n",
    "        result_E21.append(actual_fit)\n",
    "        result_E21.append(round(fit_score_numerical,2))\n",
    "\n",
    "    '''TRACING'''\n",
    "    print(predicate, argument, roleType, 'fit_score_final', round(fit_score_numerical,2) , round(fit_score_textual,2))\n",
    "\n",
    "    '''ADD IT TO THE RESULT DATASET'''\n",
    "    if not IsExist_E21:\n",
    "        with open(filename_E21, 'a') as csvfile: \n",
    "            # creating a csv writer object  \n",
    "            csvwriter = csv.writer(csvfile) \n",
    "\n",
    "            # writing the data rows  \n",
    "            csvwriter.writerow(result_E21) \n",
    "        \n",
    "    if not IsExist_E22:\n",
    "        with open(filename_E22, 'a') as csvfile: \n",
    "            # creating a csv writer object  \n",
    "            csvwriter = csv.writer(csvfile) \n",
    "\n",
    "            # writing the data rows  \n",
    "            csvwriter.writerow(result_E22) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e645c90-9e84-48b7-9628-1928273280f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
