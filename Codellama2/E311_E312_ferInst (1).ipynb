{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89da5844",
   "metadata": {},
   "source": [
    "# Experiment E311 E312"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68886320",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d66c8a-7ae0-4f34-afba-17d46fff9988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from tenacity import (retry, stop_after_attempt, wait_random_exponential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34ea20",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b7c74f-7d99-488d-8f7d-57830de55939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List that hold the records\n",
    "data = []\n",
    "\n",
    "# Opening the CSV file\n",
    "with open('Dataset/ferretti_instrument.csv', mode ='r')as file:\n",
    "   \n",
    "  # reading the CSV file\n",
    "  csvFile = csv.reader(file)\n",
    "  \n",
    "  # displaying the contents of the CSV file\n",
    "  for lines in csvFile:\n",
    "      data.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ea381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving the results \n",
    "fields_E21 = ['predicate','argument','reason1','reason2','reason3','actual_fit','exp_fit']\n",
    "filename_E21 = 'Result_final/E311_ferInst_llama13b_results.csv'\n",
    "    \n",
    "fields_E22 = ['predicate','argument','reason1','reason2','reason3','actual_fit','exp_fit']\n",
    "filename_E22 = 'Result_final/E312_ferInst_llama13b_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355332e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filename_E21):\n",
    "    with open(filename_E21, 'a') as csvfile: \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields_E21)\n",
    "\n",
    "if not os.path.exists(filename_E22):\n",
    "    with open(filename_E22, 'a') as csvfile: \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields_E22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994c6e4",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cc367a-93c4-4c84-a888-57e326108a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://0.0.0.0:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ece7cf8-4883-4147-8cf8-caec2f6d2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_engine= \"codellama/CodeLlama-13b-Instruct-hf\"\n",
    "temp = 0.0\n",
    "top_p = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79987da5-881e-4d78-a3e7-cc54dd464533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_with_functions(conversation: [{}]):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=model_engine,\n",
    "      messages=conversation,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      temperature= temp,\n",
    "      top_p = top_p,\n",
    "    )\n",
    " \n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884730a",
   "metadata": {},
   "source": [
    "### Help Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8210ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_fit_scoring(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score_textual= f\"Given the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}?  Reply only with JSON list has the key 'Fit score' containing a value that is one of 'Near-Perfect', 'High', 'Medium', 'Low' or 'Near-Impossible'. Do not reply with anything else other than the JSON list.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score_textual})\n",
    "    response_fit_score_textual =  chat_with_functions(conversation = conversation)                \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_fit_score_textual', response_fit_score_textual)\n",
    "    \n",
    "    fit_score_textual = json.loads(response_fit_score_textual)['Fit score']\n",
    "    ## CHECK\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score_textual}\"})\n",
    "    fit_score = textual_to_numerical_scale(fit_score_textual)\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90be40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_fit_scoring(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score= f\"Given the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}? Reply only with JSON list has the key 'Fit score' containing a value that is a float number from 0 to 1. Do not reply with anything else other than the JSON list.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score})\n",
    "    response_fit_score = chat_with_functions(conversation = conversation)                 \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_fit_score', response_fit_score)\n",
    "    \n",
    "    fit_score = float(json.loads(response_fit_score)['Fit score'])\n",
    "    ## CHECK\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score}\"})\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd41c3cc-7e3f-4b89-ac13-ae432dd62ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning(conversation: [{}], predicate: str, argument: str, roleType: str, result_E21: [], result_E22: []):\n",
    "    reasons = [f\"Given the predicate '{predicate}', what properties does the {roleType} role have? Reply only with JSON list has the key 'Reason' containing the answer. Do not reply with anything else other than the JSON list. \",\n",
    "                f\"Given the predicate '{predicate}', and the argument '{argument}' in the Instrument role, what relevant properties does the argument have?  Reply only with JSON list has the key 'Reason' containing the answer. Do not reply with anything else other than the JSON list. \",\n",
    "                f\"Given the above, how will the argument '{argument}' fit the {roleType} role for the predicate '{predicate}'?  Reply only with JSON list has the key 'Reason' containing the answer. Do not reply with anything else other than the JSON list. \"]\n",
    "            \n",
    "    for reason_prompt in reasons:\n",
    "        print('#############reason_prompt:#################\\n', reason_prompt)\n",
    "        conversation.append({\"role\": \"user\", \"content\": reason_prompt})\n",
    "        response_reason = chat_with_functions(conversation = conversation)                 \n",
    "        \n",
    "        '''TRACING'''\n",
    "        print('response_reason', \"\".join(response_reason))\n",
    "        \n",
    "        reason = json.loads(response_reason)['Reason']\n",
    "        reason = \"\".join(reason)\n",
    "        \n",
    "        print('**************reason:*************\\n', reason)\n",
    "        \n",
    "        result_E21.append(reason)\n",
    "        result_E22.append(reason)\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": f\"{reason}\"})\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a84af91-3441-452f-93c5-2bcbf047f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowExist(result_file_name, predicate, argument):\n",
    "    with open(result_file_name, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        for line in csvFile:\n",
    "            if predicate in line and argument in line:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3875cd-bd26-4192-87b7-3f63fd8e3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_to_numerical_scale(level):\n",
    "    if \"Near-Impossible\" in level:\n",
    "        return 0.0\n",
    "    elif \"Low\" in level:\n",
    "        return 0.25\n",
    "    elif \"Medium\" in level:\n",
    "        return 0.5\n",
    "    elif \"High\" in level:\n",
    "        return 0.75\n",
    "    elif \"Near-Perfect\" in level:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c2cf",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03e11f45-92e3-41d4-bd99-80d36dc6ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Record\n",
      "--------------------------------------\n",
      "serve plate\n",
      "#############reason_prompt:#################\n",
      " Given the predicate 'serve', what properties does the Instrument role have? Reply only with JSON list has the key 'Reason' containing the answer. Do not reply with anything else other than the JSON list. \n",
      "response_reason   [\n",
      "  {\n",
      "    \"Reason\": \"The Instrument role is typically associated with the action of serving, and is often used to describe the means by which something is served, such as a spoon or a fork.\"\n",
      "  },\n",
      "  {\n",
      "    \"Reason\": \"In the sentence 'She served the soup with a spoon', the spoon is the instrument used to serve the soup.\"\n",
      "  },\n",
      "  {\n",
      "    \"Reason\": \"In the sentence 'He served the customer with a smile', the smile is the instrument used to serve the customer.\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Prompting'''\u001b[39;00m      \n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Reasoning Part'''\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mreasoning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroleType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_E21\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_E22\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m IsExist_E22:\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''FITING SCORING PART'''\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mreasoning\u001b[0;34m(conversation, predicate, argument, roleType, result_E21, result_E22)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''TRACING'''\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_reason\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(response_reason))\n\u001b[0;32m---> 14\u001b[0m reason \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_reason\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReason\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(reason)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**************reason:*************\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, reason)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "for i in range(5):  # This must be for all datat 'len(data)' but for now just 5 records \n",
    "    \n",
    "    '''Data Extraction'''\n",
    "    predicate = data[i][0]\n",
    "    argument = data[i][1]\n",
    "    roleType = 'Instrument'\n",
    "    actual_fit = data[i][2]\n",
    "\n",
    "    '''Check if this record is exists already'''\n",
    "    IsExist_E21 = False\n",
    "    IsExist_E22 = False \n",
    "    \n",
    "    if os.path.exists(filename_E21) and  os.path.exists(filename_E22):\n",
    "        IsExist_E21 = rowExist(filename_E21, predicate,argument)\n",
    "        IsExist_E22 = rowExist(filename_E22, predicate,argument)\n",
    "\n",
    "        if IsExist_E21 and IsExist_E22:\n",
    "            continue\n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('New Record')\n",
    "    print('--------------------------------------')\n",
    "    print(predicate,argument)\n",
    "    \n",
    "    '''Result Preparation'''    \n",
    "    if not IsExist_E21:\n",
    "        result_E21 = [predicate,argument]\n",
    "    if not IsExist_E22:\n",
    "        result_E22 = [predicate,argument] \n",
    "\n",
    "    '''Chat Completion Needed Parameter'''\n",
    "    system_prompt = \"You are a linguist who can understand the semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument and the semantic role.\"\n",
    "    conversation = [{\"role\": \"system\", \"content\": system_prompt}] \n",
    "\n",
    "    '''Prompting'''      \n",
    "    \n",
    "    '''Reasoning Part'''\n",
    "    conversation = reasoning(conversation, predicate, argument, roleType, result_E21, result_E22)\n",
    "\n",
    "    if not IsExist_E22:\n",
    "        '''FITING SCORING PART'''\n",
    "        fit_score_textual= textual_fit_scoring(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "        '''TRACING'''\n",
    "        print(fit_score_textual)\n",
    "                \n",
    "        result_E22.append(actual_fit)\n",
    "        result_E22.append(round(fit_score_textual,2))\n",
    "\n",
    "    if not IsExist_E21:\n",
    "        fit_score_numerical = numerical_fit_scoring(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "        '''TRACING'''\n",
    "        print(fit_score_numerical)\n",
    "\n",
    "        result_E21.append(actual_fit)\n",
    "        result_E21.append(round(fit_score_numerical,2))\n",
    "\n",
    "    '''TRACING'''\n",
    "    print(predicate, argument, roleType, 'fit_score_final', 'textual', round(fit_score_textual,2), 'numerical', round(fit_score_numerical,2))\n",
    "\n",
    "    '''ADD IT TO THE RESULT DATASET'''\n",
    "    if not IsExist_E21:\n",
    "        with open(filename_E21, 'a') as csvfile: \n",
    "            # creating a csv writer object  \n",
    "            csvwriter = csv.writer(csvfile) \n",
    "\n",
    "            # writing the data rows  \n",
    "            csvwriter.writerow(result_E21) \n",
    "        \n",
    "    if not IsExist_E22:\n",
    "        with open(filename_E22, 'a') as csvfile: \n",
    "            # creating a csv writer object  \n",
    "            csvwriter = csv.writer(csvfile) \n",
    "\n",
    "            # writing the data rows  \n",
    "            csvwriter.writerow(result_E22) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e645c90-9e84-48b7-9628-1928273280f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
