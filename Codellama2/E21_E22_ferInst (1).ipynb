{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89da5844",
   "metadata": {},
   "source": [
    "# Experiment E21 E22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68886320",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2d66c8a-7ae0-4f34-afba-17d46fff9988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from tenacity import (retry, stop_after_attempt, wait_random_exponential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34ea20",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1b7c74f-7d99-488d-8f7d-57830de55939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List that hold the records\n",
    "data = []\n",
    "\n",
    "# Opening the CSV file\n",
    "with open('Dataset/ferretti_instrument.csv', mode ='r')as file:\n",
    "   \n",
    "  # reading the CSV file\n",
    "  csvFile = csv.reader(file)\n",
    "  \n",
    "  # displaying the contents of the CSV file\n",
    "  for lines in csvFile:\n",
    "      data.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89ea381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving the results \n",
    "fields_E21 = ['predicate','role','sentence_1','fit_score_1','sentence_2','fit_score_2','sentence_3','fit_score_3','sentence_4','fit_score_4','sentence_5','fit_score_5','actual_fit','exp_fit']\n",
    "filename_E21 = 'Result_final/E21_ferInst_llama13b_results.csv'\n",
    "    \n",
    "fields_E22 = ['predicate','role','sentence_1','fit_score_1','sentence_2','fit_score_2','sentence_3','fit_score_3','sentence_4','fit_score_4','sentence_5','fit_score_5','actual_fit','exp_fit']\n",
    "filename_E22 = 'Result_final/E22_ferInst_llama13b_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "355332e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filename_E21):\n",
    "    with open(filename_E21, 'a') as csvfile: \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields_E21)\n",
    "\n",
    "if not os.path.exists(filename_E22):\n",
    "    with open(filename_E22, 'a') as csvfile: \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile) \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields_E22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994c6e4",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a9cc367a-93c4-4c84-a888-57e326108a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://0.0.0.0:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0ece7cf8-4883-4147-8cf8-caec2f6d2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_engine= \"codellama/CodeLlama-13b-Instruct-hf\"\n",
    "temp = 0.0\n",
    "top_p = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79987da5-881e-4d78-a3e7-cc54dd464533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_with_functions(conversation: [{}]):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=model_engine,\n",
    "      messages=conversation,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      temperature= temp,\n",
    "      top_p = top_p,\n",
    "    )\n",
    " \n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884730a",
   "metadata": {},
   "source": [
    "### Help Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bdbe591b-9146-47a9-a8c9-c579280141a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_generate_sentence = f\"Generate five sentences that semantically fit the given predicate '{predicate}', argument '{argument}', and the role of {roleType}. Reply only with a JSON list containing the key 'sentences' with the five sentences each is semantically coherent with the given predicate, argument, and role. Do not reply with anything else other than the JSON list.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_generate_sentence})\n",
    "    response_generate_sentence = chat_with_functions(conversation = conversation)\n",
    "\n",
    "    '''TRACING'''\n",
    "    print('response_generate_sentence', response_generate_sentence)\n",
    "    \n",
    "    sentences = json.loads(response_generate_sentence)[\"sentences\"]\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": \",\".join(sentences)})\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "39849e48-7444-4e2e-9356-d408b294912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_coherent (conversation: [{}], sentence: str):\n",
    "    prompt_check_semantic = f\"Is the given sentence '{sentence} semantically coherent? Reply only with JSON list has the key 'Is Fit' containing 'Yes' or 'No'. Do not reply with anything else other than the JSON list.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_check_semantic})\n",
    "    response_check_semantic = chat_with_functions(conversation = conversation) \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_check_semantic', response_check_semantic)\n",
    "    \n",
    "    is_json_format = is_json(response_check_semantic)\n",
    "    \n",
    "    if is_json_format:\n",
    "        check_semantic = json.loads(response_check_semantic)[\"Is Fit\"]      \n",
    "    else:\n",
    "        check_semantic = response_check_semantic\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{check_semantic}\"})\n",
    "\n",
    "    if 'No' in response_check_semantic:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "77d54f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_fit_scoring_with_senetence(conversation: [{}], sentence: str, predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score_textual= f\"Given the following sentence '{sentence}', for the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}?  Reply only with JSON list has the key 'Fit score' containing a value that is one of 'Near-Perfect', 'High', 'Medium', 'Low' or 'Near-Impossible'. Do not reply with anything else other than the JSON list.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score_textual})\n",
    "    response_fit_score_textual =  chat_with_functions(conversation = conversation)                \n",
    "\n",
    "    '''TRACING'''\n",
    "    print('response_fit_score_textual',response_fit_score_textual)\n",
    "    \n",
    "    is_json_format = is_json(response_fit_score_textual)\n",
    "    \n",
    "    if is_json_format:\n",
    "        fit_score_textual = json.loads(response_fit_score_textual)['Fit score']    \n",
    "    else:\n",
    "        fit_score_textual = response_fit_score_textual\n",
    "    \n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score_textual}\"})\n",
    "    fit_score = textual_to_numerical_scale(fit_score_textual)\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f5c4f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_fit_scoring_with_senetence(conversation: [{}], sentence: str, predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score= f\"Given the following sentence '{sentence}', for the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}?  Reply only with JSON list has the key 'Fit score' containing a value that is a float number from 0 to 1. Do not reply with anything else other than the JSON list. \"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score})\n",
    "    response_fit_score =  chat_with_functions(conversation = conversation)                \n",
    "\n",
    "    '''TRACING'''\n",
    "    print('response_fit_score', response_fit_score)\n",
    "    \n",
    "    is_json_format = is_json(response_fit_score)\n",
    "    \n",
    "    print('jjjj', is_json_format)\n",
    "        \n",
    "    if is_json_format:\n",
    "        fit_score = json.loads(response_fit_score)['Fit score']    \n",
    "    else:\n",
    "        fit_score = response_fit_score\n",
    "    \n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score}\"})\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8210ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_fit_scoring_with_backoff(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score_textual= f\"Given the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}?  Reply only with JSON list has the key 'Fit score' containing a value that is one of 'Near-Perfect', 'High', 'Medium', 'Low' or 'Near-Impossible'. Do not reply with anything else other than the JSON list.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score_textual})\n",
    "    response_fit_score_textual =  chat_with_functions(conversation = conversation)                \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_fit_score_textual', response_fit_score_textual)\n",
    "    \n",
    "    is_json_format = is_json(response_fit_score_textual)\n",
    "    \n",
    "    if is_json_format:\n",
    "        fit_score_textual = json.loads(response_fit_score_textual)['Fit score']\n",
    "    else:\n",
    "        fit_score_textual = response_fit_score_textual\n",
    "        \n",
    "    ## CHECK\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score_textual}\"})\n",
    "    \n",
    "    fit_score = textual_to_numerical_scale(fit_score_textual)\n",
    "    fit_score /= 2\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "90be40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_fit_scoring_with_backoff(conversation: [{}], predicate: str, argument: str, roleType: str):\n",
    "    prompt_fit_score= f\"Given the predicate '{predicate}', how much does the argument '{argument}' fit the role of {roleType}? Reply only with JSON list has the key 'Fit score' containing a value that is a float number from 0 to 1. Do not reply with anything else other than the JSON list.\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt_fit_score})\n",
    "    response_fit_score = chat_with_functions(conversation = conversation)                 \n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('response_fit_score', response_fit_score)\n",
    "    \n",
    "    is_json_format = is_json(response_fit_score)\n",
    "    \n",
    "    if is_json_format:\n",
    "        fit_score = float(json.loads(response_fit_score)['Fit score'])\n",
    "    else:\n",
    "        fit_score = float(response_fit_score_textual)\n",
    "        \n",
    "    conversation.append({\"role\": \"assistant\", \"content\": f\"{fit_score}\"})\n",
    "    fit_score /= 2\n",
    "\n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "afe3a4b3-827f-4e4c-a305-c695a6a8df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_json(myjson):\n",
    "    try: json.loads(myjson)\n",
    "    except ValueError as e: \n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8a84af91-3441-452f-93c5-2bcbf047f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowExist(result_file_name, predicate, argument):\n",
    "    with open(result_file_name, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        for line in csvFile:\n",
    "            if predicate in line and argument in line:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ae3875cd-bd26-4192-87b7-3f63fd8e3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_to_numerical_scale(level):\n",
    "    if \"Near-Impossible\" in level:\n",
    "        return 0.0\n",
    "    elif \"Low\" in level:\n",
    "        return 0.25\n",
    "    elif \"Medium\" in level:\n",
    "        return 0.5\n",
    "    elif \"High\" in level:\n",
    "        return 0.75\n",
    "    elif \"Near-Perfect\" in level:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c2cf",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "03e11f45-92e3-41d4-bd99-80d36dc6ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Record\n",
      "--------------------------------------\n",
      "serve plate\n",
      "response_generate_sentence   {\n",
      "\"sentences\": [\n",
      "\"The waiter served the plate of spaghetti to the customer.\",\n",
      "\"The chef served the plate of sushi to the customer.\",\n",
      "\"The server served the plate of fries to the customer.\",\n",
      "\"The waiter served the plate of soup to the customer.\",\n",
      "\"The chef served the plate of salad to the customer.\"\n",
      "]\n",
      "}\n",
      "['The waiter served the plate of spaghetti to the customer.', 'The chef served the plate of sushi to the customer.', 'The server served the plate of fries to the customer.', 'The waiter served the plate of soup to the customer.', 'The chef served the plate of salad to the customer.']\n",
      "response_check_semantic   {\n",
      "\"Is Fit\": \"Yes\"\n",
      "}\n",
      "response_fit_score_textual  {\n",
      "\"Fit score\": \"High\"\n",
      "}\n",
      "0.75\n",
      "response_fit_score  {\n",
      "\"Fit score\": 0.8\n",
      "}\n",
      "jjjj True\n",
      "0.8\n",
      "response_check_semantic   Yes\n",
      "response_fit_score_textual   High\n",
      "0.75\n",
      "response_fit_score  0.9\n",
      "jjjj True\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m     avg_E22 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m fit_score\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m IsExist_E21:\n\u001b[0;32m--> 103\u001b[0m     fit_score \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_fit_scoring_with_senetence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroleType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroleType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''TRACING'''\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(fit_score)\n",
      "Cell \u001b[0;32mIn[110], line 14\u001b[0m, in \u001b[0;36mnumerical_fit_scoring_with_senetence\u001b[0;34m(conversation, sentence, predicate, argument, roleType)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjjjj\u001b[39m\u001b[38;5;124m'\u001b[39m, is_json_format)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_json_format:\n\u001b[0;32m---> 14\u001b[0m     fit_score \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_fit_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFit score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m    \n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     fit_score \u001b[38;5;241m=\u001b[39m response_fit_score\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for i in range(5):  # This must be for all datat 'len(data)' but for now just 5 records \n",
    "    \n",
    "    '''Data Extraction'''\n",
    "    predicate = data[i][0]\n",
    "    argument = data[i][1]\n",
    "    roleType = 'Instrument'\n",
    "    actual_fit = data[i][2]\n",
    "\n",
    "    '''Check if this record is exists already'''\n",
    "    IsExist_E21 = False\n",
    "    IsExist_E22 = False \n",
    "    \n",
    "    if os.path.exists(filename_E21) and  os.path.exists(filename_E22):\n",
    "        IsExist_E21 = rowExist(filename_E21, predicate,argument)\n",
    "        IsExist_E22 = rowExist(filename_E22, predicate,argument)\n",
    "\n",
    "        if IsExist_E21 and IsExist_E22:\n",
    "            continue\n",
    "    \n",
    "    '''TRACING'''\n",
    "    print('New Record')\n",
    "    print('--------------------------------------')\n",
    "    print(predicate,argument)\n",
    "    \n",
    "    '''Result Preparation'''    \n",
    "    if not IsExist_E21:\n",
    "        result_E21 = [predicate,argument]\n",
    "    if not IsExist_E22:\n",
    "        result_E22 = [predicate,argument] \n",
    "\n",
    "    '''Chat Completion Needed Parameter'''\n",
    "    system_prompt = \"You are a linguist who can understand the semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument and the semantic role.\"\n",
    "    conversation = [{\"role\": \"system\", \"content\": system_prompt}] \n",
    "\n",
    "    '''Sentence Generation Part'''\n",
    "    sentence_no = 5\n",
    "    sentences = generate_sentence(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "    '''TRACING'''\n",
    "    print(sentences)\n",
    "    \n",
    "    '''Prompting'''\n",
    "    avg_E21 = 0\n",
    "    avg_E22 = 0\n",
    "\n",
    "    for j in sentences:\n",
    "\n",
    "        '''Checking Semantic Coherent'''\n",
    "        is_semantic = semantic_coherent(conversation= conversation, sentence= j)      \n",
    "            \n",
    "        '''Not Sematically Fit Sentence'''\n",
    "        if not is_semantic:\n",
    "\n",
    "            '''Back Off'''\n",
    "            \n",
    "            '''TRACING'''\n",
    "            print(j, '\\t Not Sematically Fit Sentence')\n",
    "            \n",
    "            # as the sentnece is not semantically fit, do not consider and add it as an empty in the result\n",
    "            if not IsExist_E21:\n",
    "                result_E21.append('')\n",
    "            if not IsExist_E22:\n",
    "                result_E22.append('')\n",
    "                    \n",
    "            if not IsExist_E22:\n",
    "                fit_score = textual_fit_scoring_with_backoff(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "                '''TRACING'''\n",
    "                print(fit_score)\n",
    "                \n",
    "                result_E22.append(fit_score)\n",
    "                avg_E22 += fit_score\n",
    "\n",
    "            if not IsExist_E21:\n",
    "                fit_score = numerical_fit_scoring_with_backoff(conversation= conversation, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "                '''TRACING'''\n",
    "                print(fit_score)\n",
    "                \n",
    "                result_E21.append(fit_score)\n",
    "                avg_E21 += fit_score\n",
    "\n",
    "        '''Sematically Fit Sentence'''\n",
    "        if is_semantic:    \n",
    "\n",
    "            \n",
    "            # consider the sentence\n",
    "            if not IsExist_E21:\n",
    "                result_E21.append(j)\n",
    "            if not IsExist_E22:\n",
    "                result_E22.append(j)\n",
    "\n",
    "            if not IsExist_E22:\n",
    "                fit_score = textual_fit_scoring_with_senetence(conversation= conversation, sentence= j, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "                '''TRACING'''\n",
    "                print(fit_score)\n",
    "                \n",
    "                result_E22.append(fit_score)\n",
    "                avg_E22 += fit_score\n",
    "\n",
    "            if not IsExist_E21:\n",
    "                fit_score = numerical_fit_scoring_with_senetence(conversation= conversation, sentence= j, predicate= predicate, argument= argument, roleType=roleType)\n",
    "\n",
    "                '''TRACING'''\n",
    "                print(fit_score)\n",
    "                \n",
    "                result_E21.append(fit_score)\n",
    "                avg_E21 += fit_score\n",
    "\n",
    "    '''Add the results'''\n",
    "    result_E21.append(actual_fit)\n",
    "    result_E21.append(round(avg_E21/sentence_no,2))\n",
    "\n",
    "    result_E22.append(actual_fit)\n",
    "    result_E22.append(round(avg_E22/sentence_no,2))\n",
    "\n",
    "    '''TRACING'''\n",
    "    print(predicate, argument, roleType, 'fit_score_final', round(avg_E21/sentence_no,2) ,  round(avg_E22/sentence_no,2))\n",
    "\n",
    "    '''ADD IT TO THE RESULT DATASET'''\n",
    "    if not IsExist_E21:\n",
    "        with open(filename_E21, 'a') as csvfile: \n",
    "            # creating a csv writer object  \n",
    "            csvwriter = csv.writer(csvfile) \n",
    "\n",
    "            # writing the data rows  \n",
    "            csvwriter.writerow(result_E21) \n",
    "        \n",
    "    if not IsExist_E22:\n",
    "        with open(filename_E22, 'a') as csvfile: \n",
    "            # creating a csv writer object  \n",
    "            csvwriter = csv.writer(csvfile) \n",
    "\n",
    "            # writing the data rows  \n",
    "            csvwriter.writerow(result_E22) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b9995-66aa-4461-ad2e-45f1bb0a9c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
