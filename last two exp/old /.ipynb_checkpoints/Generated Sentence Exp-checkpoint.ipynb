{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Sentences Expeirments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import models\n",
    "import experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create The Model API \n",
    "model_name = \"codellama/CodeLlama-13b-Instruct-hf\"\n",
    "model_key = \"EMPTY\"\n",
    "model_base_url = \"http://0.0.0.0:8000/v1\"\n",
    "model_temp = 0.0\n",
    "model_top_p = 0.95\n",
    "model_max_tokens = 100\n",
    "\n",
    "llama2_model = models.Model(model_name,model_key,model_base_url,model_temp, model_top_p, model_max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 2.1 & 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversation [{'role': 'system', 'content': 'You are a linguist who understands semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument, and the semantic role.'}]\n",
      "the record is missing in either in E11 False or in False\n",
      "New Record: serve plate\n",
      "Generate five semantically coherent sentences with the predicate 'serve', argument 'plate', and the role of Instrument. Reply only with a valid JSON object containing the key 'Sentences' and a value that is a list of  five semantically coherent sentences, each with the given predicate, argument, and role in the following the structure {'Sentences': [String]}. Avoid adding any text outside this JSON object.\n",
      "conversation before: [{'role': 'system', 'content': 'You are a linguist who understands semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument, and the semantic role.'}]\n",
      "response_generate_sentence   {\n",
      "\"Sentences\": [\n",
      "\"The waiter served the food on the plate.\",\n",
      "\"The chef served the soup in the bowl.\",\n",
      "\"The baker served the cake on the plate.\",\n",
      "\"The chef served the salad in the bowl.\",\n",
      "\"The waiter served the drink in the glass.\"\n",
      "]\n",
      "}\n",
      "Is the given sentence 'The waiter served the food on the plate.' semantically coherent and containing the given predicate  'serve' and the argument 'plate' in the role of Instrument? Reply only with a valid JSON object containing the key 'Is Fit' and the value 'Yes' or 'No' in the following structure {'Is Fit': String}. Avoid adding any text outside this JSON object.\n",
      "conversation before: [{'role': 'system', 'content': 'You are a linguist who understands semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument, and the semantic role.'}]\n",
      "response_check_semantic   {\n",
      "\"Is Fit\": \"Yes\"\n",
      "}\n",
      "{'Is Fit': 'Yes'}\n",
      " \n",
      "\n",
      "\n",
      " new senetence  [{'role': 'system', 'content': 'You are a linguist who understands semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument, and the semantic role.'}, {'role': 'user', 'content': \"Is the given sentence 'The waiter served the food on the plate.' semantically coherent and containing the given predicate  'serve' and the argument 'plate' in the role of Instrument? Reply only with a valid JSON object containing the key 'Is Fit' and the value 'Yes' or 'No' in the following structure {'Is Fit': String}. Avoid adding any text outside this JSON object.\"}, {'role': 'assistant', 'content': \"{'Is Fit': 'Yes'}\"}] \n",
      "\n",
      "\n",
      "\n",
      "Given the following sentence, 'The waiter served the food on the plate.', for the predicate 'serve', how much does the argument 'plate' fit the role of Instrument? Reply only with a valid JSON object containing the key 'Fit score' and a value that is one of 'Near-Perfect', 'High', 'Medium', 'Low' or 'Near-Impossible', following the structure {'Fit score': String}. Avoid adding any text outside this JSON object.\n",
      "conversation before: [{'role': 'system', 'content': 'You are a linguist who understands semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument, and the semantic role.'}, {'role': 'user', 'content': \"Is the given sentence 'The waiter served the food on the plate.' semantically coherent and containing the given predicate  'serve' and the argument 'plate' in the role of Instrument? Reply only with a valid JSON object containing the key 'Is Fit' and the value 'Yes' or 'No' in the following structure {'Is Fit': String}. Avoid adding any text outside this JSON object.\"}, {'role': 'assistant', 'content': \"{'Is Fit': 'Yes'}\"}]\n",
      "response_fit_score   {'Fit score': 'High'}\n",
      "Invalid JSON syntax: Expecting property name enclosed in double quotes: line 1 column 4 (char 3)\n",
      "None\n",
      "conversation after: [{'role': 'system', 'content': 'You are a linguist who understands semantic roles and can provide a rating on the semantic fit of predicate-arguments for a specific semantic role, given the predicate, the argument, and the semantic role.'}, {'role': 'user', 'content': \"Is the given sentence 'The waiter served the food on the plate.' semantically coherent and containing the given predicate  'serve' and the argument 'plate' in the role of Instrument? Reply only with a valid JSON object containing the key 'Is Fit' and the value 'Yes' or 'No' in the following structure {'Is Fit': String}. Avoid adding any text outside this JSON object.\"}, {'role': 'assistant', 'content': \"{'Is Fit': 'Yes'}\"}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_gen_sentences_ferretti\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mferretti_instrument.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInstrument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllama2_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/new_design/experiments.py:223\u001b[0m, in \u001b[0;36mexp_gen_sentences_ferretti\u001b[0;34m(filename, role_type, model, model_name)\u001b[0m\n\u001b[1;32m    220\u001b[0m result1\u001b[38;5;241m.\u001b[39mappend(sentence)\n\u001b[1;32m    221\u001b[0m result2\u001b[38;5;241m.\u001b[39mappend(sentence)\n\u001b[0;32m--> 223\u001b[0m fit_score \u001b[38;5;241m=\u001b[39m \u001b[43mfit_scoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_with_senetence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroleType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''TRACING'''\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_with_senetence\u001b[39m\u001b[38;5;124m'\u001b[39m,fit_score)\n",
      "File \u001b[0;32m~/new_design/fit_scoring.py:70\u001b[0m, in \u001b[0;36mcategorical_with_senetence\u001b[0;34m(model, predicate, argument, roleType, sentence)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconversation after:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mconversation)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Convert it to a float number\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m fit_score \u001b[38;5;241m=\u001b[39m \u001b[43mtextual_to_numerical_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fit_score\n",
      "File \u001b[0;32m~/new_design/fit_scoring.py:134\u001b[0m, in \u001b[0;36mtextual_to_numerical_scale\u001b[0;34m(level)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtextual_to_numerical_scale\u001b[39m(level):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNear-Impossible\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m level:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "experiments.exp_gen_sentences_ferretti('ferretti_instrument.csv','Instrument', llama2_model ,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 3.1.1 & 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments.exp_lemma_tuple_reasoning_ferretti('ferretti_instrument.csv', 'Instrument', llama2_model ,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llama2_model.conversa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
